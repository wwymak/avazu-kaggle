{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model building on subset of avazu data\n",
    "\n",
    "This notebook is for building a model to predict whether an ad will get clicked on , given features around ad placement, when/how it's seen etc. Using the subset (500000 samples) to explore which model(s) do best before running on the large dataset.\n",
    "\n",
    "Process:\n",
    "- use the data subset\n",
    "- one hot encode features\n",
    "- test out several sklearn models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') #stop those annoying deprecation warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer, LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from scipy import stats\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('train_subset_df.pkl')\n",
    "X = data.drop(['click', 'hour', 'day', 'month'], axis=1)\n",
    "X = pd.get_dummies(columns = X.columns, data=X)\n",
    "y = data['click']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['C1_1001', 'C1_1002', 'C1_1005', 'C1_1007', 'C1_1008', 'C1_1010',\n",
       "       'C1_1012', 'banner_pos_0', 'banner_pos_1', 'banner_pos_2',\n",
       "       'banner_pos_4', 'banner_pos_5', 'banner_pos_7', 'device_type_0',\n",
       "       'device_type_1', 'device_type_4', 'device_type_5', 'device_conn_type_0',\n",
       "       'device_conn_type_2', 'device_conn_type_3', 'device_conn_type_5',\n",
       "       'C15_120', 'C15_216', 'C15_300', 'C15_320', 'C15_480', 'C15_728',\n",
       "       'C15_768', 'C15_1024', 'C16_20', 'C16_36', 'C16_50', 'C16_90',\n",
       "       'C16_250', 'C16_320', 'C16_480', 'C16_768', 'C16_1024', 'C18_0',\n",
       "       'C18_1', 'C18_2', 'C18_3', 'hour_of_day_0', 'hour_of_day_1',\n",
       "       'hour_of_day_2', 'hour_of_day_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modelling algorithms:\n",
    "Try out one each from few different families:\n",
    "\n",
    "- xgboost classifier (ensembles)\n",
    "- support vector machine (SVM)\n",
    "- SGDClassifier (linear model)\n",
    "- multinomial NB (gaussian)\n",
    "- KNN classifier (neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrC = LogisticRegression()\n",
    "knnC = KNeighborsClassifier()\n",
    "svcC = SVC(kernel='rbf', C=1e3, gamma=0.5)\n",
    "xgbC = xgb.XGBClassifier(**{\n",
    "   \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_depth\": 5, \n",
    "    \"min_child_weight\": 1,\n",
    "    \"random_state\": 10\n",
    "})\n",
    "nbC = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking function, which also prints out the confusion matrix + classification report. Also calculates the\n",
    "# accuracy and the roc_Score\n",
    "def benchmark(clf, X_train, y_train, X_val, y_val):\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_val)\n",
    "    accuracy = metrics.accuracy_score(y_val, prediction)\n",
    "    logloss = metrics.log_loss(y_val, prediction)\n",
    "    clf_description = str(clf).split('(')[0]\n",
    "    print(clf_description)\n",
    "    print(\"logloss: {}\".format(logloss))\n",
    "    print(metrics.confusion_matrix(y_val, prediction))\n",
    "    \n",
    "    return clf, clf_description, accuracy, logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "logloss: 5.952777230037412\n",
      "[[81146  2391]\n",
      " [14844  1619]]\n",
      "LogisticRegression\n",
      "logloss: 5.686118757894146\n",
      "[[83537     0]\n",
      " [16463     0]]\n",
      "SVC\n",
      "logloss: 5.687500436885531\n",
      "[[83521    16]\n",
      " [16451    12]]\n",
      "XGBClassifier\n",
      "logloss: 5.686464241609786\n",
      "[[83525    12]\n",
      " [16452    11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wwymak/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "models = [nbC, lrC, svcC, xgbC]\n",
    "for model in models:\n",
    "    benchmark(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately, it seems like none of the classifiers do particularly well. This could either be that we are ignoring features that are important, (especially since we are using a small subset of the avaiable features) or the hyperparmaters are not well tuned at all, or none of these models work well. \n",
    "\n",
    "Some approaches to try:\n",
    "- hyperparam tuning on the whole dataset (with the feature subset)\n",
    "- retry models with all the features (potentially using sklearn's feature selection functions/featurtools)\n",
    "- one of the models that have seen a lot of success in CTR prediction is factorisation machines, should probably try this.\n",
    "- use entity embeddings as features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
